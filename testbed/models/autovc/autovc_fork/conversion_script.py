"""

"""
import argparse 
import os
import pickle
from math import ceil
import numpy as np
import shlex
import shutil
import soundfile as sf
import subprocess
import torch

from model_vc import Generator
from synthesis import build_model, wavegen

### State and setup management functions ###
def build_argument_parser():
	parser = argparse.ArgumentParser(
		description="CLI parser for AutoVC experimentation."
	)
	parser.add_argument(
		"-s",
		"--source-wav",
		dest="source_wav",
		default=None,
		action="store",
		type=str,
		help="The source wav, from which speaker embeddings and content will be extracted."
	)
	parser.add_argument(
		"-e",
		"--source-embedding",
		dest="source_embedding",
		default=None,
		action="store",
		type=str,
		help="A source embedding (optional substitute for the embedding generated by AutoVC from the source wav)."
	)
	parser.add_argument(
		"-t",
		"--target-embedding",
		dest="target_embedding",
		default=None,
		action="store",
		type=str,
		help="A target embedding (optional substitute for the embedding generated by AutoVC from the destination wav)."
	)
	parser.add_argument(
		"-d",
		"--destination-wav",
		dest="destination_wav",
		default=None,
		action="store",
		type=str,
		help="A source embedding (optional substitute for the embedding generated by AutoVC)."
	)
	parser.add_argument(
		"-m",
		"--conversion-model",
		dest="conversion_model",
		default=None,
		action="store",
		type=str,
		help="The conversion model (default AutoVC)."
	)
	parser.add_argument(
		"-v",
		"--vocoder-model",
		dest="vocoder_model",
		default=None,
		action="store",
			type=str,
			help="The vocoder model (default WaveNET)."
	)
	return parser

def validate_args(args):
    for required in ["source_wav", "destination_wav"]:
        if not getattr(args, required):
            print(f"Error: missing or invalid value for {required}.")
            exit()
    if args.source_embedding:
        try:
            np.load(args.source_embedding)
        except:
            print(f"Error: cannot read source embedding as a .npy file.")
            exit()
    if args.target_embedding:
        try:
    	    np.load(args.target_embedding)
        except:
            print(f"Error: cannot read target embedding as a .npy file.")
            exit()
    if not args.vocoder_model:  
        if not os.path.isfile("wavenet.pth"):
            print(f"Error: cannot find wavenet.pth or any other vocoder model.")
            exit()
    if not args.conversion_model:
        if not os.path.isfile("autovc.ckpt"):
            print(f"Error: cannot find autovc.ckpt or any other vocoder model.")
            exit()
    return

def vocode(pkl, vocoder):
    spect_vc = pickle.load(open(pkl, 'rb'))
    device = torch.device("cuda")
    model = build_model().to(device)
    checkpoint = torch.load(vocoder)
    model.load_state_dict(checkpoint["state_dict"])

    for spect in spect_vc:
        name = spect[0]
        print("Saving output to {}.wav".format(name))
        c = spect[1]
        waveform = wavegen(model, c=c)   
        sf.write(name+'.wav', waveform, 16000)

if __name__ == '__main__':
	
    # Setup and validate arguments
    parser = build_argument_parser()
    args = parser.parse_args()
    validate_args(args)

    # Prepare directory for input to make_spect.py
    if os.path.isdir('davistf1_wavs'):
        shutil.rmtree('davistf1_wavs')
    os.mkdir('davistf1_wavs')
    os.mkdir('davistf1_wavs/s001')
    os.mkdir('davistf1_wavs/t002')
    source = args.source_wav.split("/")[-1]
    dest = args.destination_wav.split("/")[-1]
    shutil.copyfile(args.source_wav, f'davistf1_wavs/s001/{source}')
    if args.destination_wav:
        shutil.copyfile(args.destination_wav, f'davistf1_wavs/t002/{dest}')
	
    # Generate spectrogram, speaker embeddings 
    cmd = 'python3 davistf1_make_spect.py'
    cmds = shlex.split(cmd)
    p = subprocess.Popen(cmds, start_new_session=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
    p.wait()

    cmd = 'python3 davistf1_make_metadata.py'
    cmds = shlex.split(cmd)
    p = subprocess.Popen(cmds, start_new_session=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
    p.wait()
    metadata = pickle.load(open('davistf1_spmel/davistf1_metadata.pkl', 'rb'))
    
    # Build conversion metadata file
    source_emb = source.replace(".wav", ".npy")
    source_embedding = args.source_embedding if args.source_embedding else f"davistf1_spmel/s001/{source_emb}"
    dest_emb = dest.replace(".wav", ".npy")
    dest_embedding = args.target_embedding if args.target_embedding else f"davistf1_spmel/t002/{dest_emb}"
    source_metadata = [
        "s001",
        metadata[0][1],
        np.load(source_embedding) 
    ]
    target_metadata = [
	"t002",
	metadata[1][1],
	np.load(dest_embedding)
    ]
    conversion_metadata = [source_metadata, target_metadata]
	
    # Run conversion
    model = args.conversion_model if args.conversion_model else "autovc.ckpt"
    def pad_seq(x, base=32):
        len_out = int(base * ceil(float(x.shape[0])/base))
        len_pad = len_out - x.shape[0]
        assert len_pad >= 0
        return np.pad(x, ((0,len_pad),(0,0)), 'constant'), len_pad

    device = 'cuda:0'
    G = Generator(32,256,512,32).eval().to(device)
    g_checkpoint = torch.load(model, map_location='cuda:0')
    G.load_state_dict(g_checkpoint['model'])

    spect_vc = []
    for sbmt_i in conversion_metadata:
        x_org = sbmt_i[2]
        x_org, len_pad = pad_seq(x_org)
        uttr_org = torch.from_numpy(x_org[np.newaxis, :, :]).to(device)
        emb_org = torch.from_numpy(sbmt_i[1][np.newaxis, :]).to(device)
        for sbmt_j in conversion_metadata:
            if sbmt_i[0] == sbmt_j[0]: # skipping reflexive conversion
                continue
            elif sbmt_i[0] == "t002":  # skipping target -> source conversion
                continue
            emb_trg = torch.from_numpy(sbmt_j[1][np.newaxis, :]).to(device)
            with torch.no_grad():
                _, x_identic_psnt, _ = G(uttr_org, emb_org, emb_trg)
            if len_pad == 0:    
                uttr_trg = x_identic_psnt[0, 0, :, :].cpu().numpy()
            else:
                uttr_trg = x_identic_psnt[0, 0, :-len_pad, :].cpu().numpy()
            spect_vc.append( ('{}x{}'.format(sbmt_i[0], sbmt_j[0]), uttr_trg) )
    
    target_pickle = 'conversion_results.pkl'
    with open(target_pickle, 'wb') as handle:
        pickle.dump(spect_vc, handle)	 
        print("Conversion success!") 	
	
    # Clean up temporary directories and files
    shutil.rmtree("davistf1_spmel", ignore_errors=True)
    shutil.rmtree("davistf1_wavs", ignore_errors=True)
    
    # Pass to the vocoder
    vocoder_path = args.vocoder_model if args.vocoder_model else "wavenet.pth"
    vocode(target_pickle, vocoder_path)
    print("Vocoder success!")

